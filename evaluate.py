def longest_common_token_sequence(left: list, right: list, i_left: int, i_right: int) -> int:
    """
    Implementation of Longest Common Subsequence using lists of tokens
    instead of sring, hence Longest Common Token Sequence.

    Parameters
    ----------
    left: list
        First first of tokens
    right: list
        Second list of tokens
    i_left: int
        Index for iterating over left list
    i_right: int
        Index for iterating over right list

    Returns
    -------
    int
        Size of the Logest Common Token Sequence found.
    """

    if i_left < 0 or i_right < 0:
        return 0

    if left[i_left] == right[i_right]:
        return 1 + longest_common_token_sequence(left, right, i_left - 1, i_right - 1)

    return max(
        longest_common_token_sequence(left, right, i_left, i_right - 1),
        longest_common_token_sequence(left, right, i_left - 1, i_right)
    )

def evaluate_sentence(pred_tokens: list, true_tokens: list) -> (int, int):
    """
    Calculates precision and coverage of tokens.
    Based on the Longest Common Subsequence Algorithm
    to evaluate pred_tokens and true_tokens, with the difference of
    not considering spaced substrings as matches.

    Examples
    --------
    >>> evaluate_sentence(["Oi", ":)"], ["Oi", ":)"])
    (2, 0, 0)

    Parameters
    ----------
    pred_tokens: list
        List of tokens generated by the tokenizer
    true_tokens: list
        List of true tokens

    Returns
    -------
    (int, int, int)
        True positive, false positives, false negatives
    """

    true_positives = longest_common_token_sequence(pred_tokens, true_tokens,
                                                   len(pred_tokens) - 1,
                                                   len(true_tokens) - 1)
    false_positives = len(pred_tokens) - true_positives
    false_negatives = len(true_tokens) - true_positives

    return true_positives, false_positives, false_negatives
